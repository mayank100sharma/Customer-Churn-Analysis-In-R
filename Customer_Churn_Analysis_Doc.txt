Introduction

Logistic Regression is an unsupervised learning techniques which is a special type of regression used when the dependent variables are dichotomous. By the term dichotomous, we mean the dependent variables are binary in nature. Since, all the regression technique have an objective of performing predictive analytics, logistic regression also works on the same principle of predicting the outcome of a problem in near future. In Linear regression, the expected values are based on the amalgamation of the values which are taken from the predictors, whereas, in Logistic Regression, the Odds of the response are based on the amalgamation of the values which are taken by the predictors. Since, this regression consists of binary nature, the data will be generally coded in 1 or 0 form, 1 being the “True” value and 0 being the “False” value. 
The model which will be best fit would define the relationship of the binary nature of the dependent variable and the independent variable set. In this technique, the coefficient of the formula are generated that in result predict the logit transformation of characteristics probability. Mathematically, the multiple linear regression is estimated as Logit(P) value for ‘i’ = 1…n as:-
 
The amount of variance is increased while fitting the model of Logistic Regression explained in the Log odds as R2. The generalizability of the model apart from the model data which is built is reduced if we keep on adding more and more variables to the model. This problem is generally called Overfitting problem.
Decision Tree algorithm is a machine learning algorithm is mostly used when we have to take a decision in s real problem and arrive at a solution. Many decision trees analogies can be implemented in the real situation which molds the decision taken in the business environment. It is usually performed when a problem is to be solved with Classification and Regression techniques. If we talk about the structure of the decision tree, it starts with one node and ends with multiple nodes which represent a tree-like representation which results in the decision made in the end.
Random Forest algorithm produces a very accurate result when the hyper-parametric tuning is not implemented. As the Decision Tree algorithm, this is also used when Classification and Regression has to be implemented. It works on a Bagging method which means that it works on multiple models simultaneously and aggregates them resulting in a more effective and efficient outcome. This algorithm makes performing regression very effective in real data which makes it one of the mostly used algorithms for this technique. Also, the classification technique is performed really well with this algorithm. The basic idea behind its procedure is that it searches the best characteristics among random subset instead of searching best characteristics while splitting a node.
In this project, we are performing all the three algorithm discussed above on the “Telco-Customer-Churn” dataset. This dataset is related to the customer churn situation which occurs when the customers do not trade or stop any business with the organization. This situation more often happens in the industry like Telecom industry, where the users have so many options to continue their services. By performing Logistic Regression, Decision Tree and Random Forest algorithm, we are predicting the customer churn using the Telecom dataset.

Methodology
In this project, we are performing Logistic Regression, Decision Tree algorithm and the Random Tree algorithm on the Telecom Dataset to predict the customer churn in the recent future. The first step would be to load the dataset and storing it in a vector. The dataset cleaning would be done after loading the dataset. The dataset would then be divided into the training and the test dataset which is a basic method to perform analysis on a dataset. Logistic regression technique would be applied on the train data and create the model. Then this model will be compared with the raw test data to predict the outcome. The next step would be applying the Decision Tree algorithm to take the decision of customer churn condition. After applying the Decision Tree, we will perform Random forest method which searches the random characteristics of the customers at the same time and obtain the best fit outcome for the customer churn situation. After performing all the three models, the final accuracy among all the three will be compared on the basis of the accuracy rate.







Project Analysis
First of all, we need some packages to be loaded for our project. The packages are as follows:
“plyr”, “ggplot2”, “caret”, “randomForest” and “party”. We can load these packages using “library()” command.
Importing the dataset
To perform Logistic Regression, Decision Tree, and Random Forest, we are going to use telecom dataset. 
We use “read.csv” to import the .csv file into the data frame we created called as “Telco”.
Exploring the Dataset
The “str” function gives the structure of the data. As seen above, there are 7043 observations (rows) and 21 variables (columns). We are going to predict customer churn, the “Churn” variable is our target variable. All the other variables will be used as object variables in our three models.
Cleaning the dataset
Data cleaning starts with dealing with the missing values.
The “sapply” function is used to find out the missing values in columns. After applying “sapply” function on “Telco”, we found out 11 missing values in “TotalCharges” column. We need to remove these missing values as it affects the efficiency of our models.
We also need to remove those variables as well which we do not need for the analysis, like “customerID”.
Logistic Regression
To fit the model into the dataset, first we need split our data into two parts. “train” set and “test” set. The “train” set consist of 70% of the “Telco” dataset and “test” set consist of 30%.
To split the dataset, we use “createDataPartition” function, and kept 70% of the Churn variable data in a vector “trn”.
Fitting the model
The next step is to fit the logistic regression model into our data. For this we use “glm” function.
“glm” function is used to generalize linear models, specified by giving a symbolic description of the linear predictor and a description of the error distribution.
After that we use “anova()” function to compare nested models. ANOVA is a statistical technique for investigating the data by comparing the means of the subsets of the data.
After analyzing the deviance table, we can see the drop-in deviance hen adding each variable one at a time. Adding “InternetService”, “Contract” and “Tenure” variables significantly reduces the residual deviance.
After that we need to evaluate the Logistic Regression Model predictive ability, it gives the accuracy of our Logistic Regression model, which comes out to be 0.7998. 
We use confusion matrix which calculates a cross-tabulation of observed and predicted classes with associated statistics. The function requires that the factors have exactly the same levels.
Decision Tree
After fitting the Logistic Regression Model, now we visualize the data using Decision Tree Model.
To visualize in a better way, we will use 3 variables “Contract”, “tenure” and “PaperlessBilling”.
We created a decision tree using a “ctree” function. The following is the decision tree visualization of dependent variable Churn and independent variables “Contract”, “tenure” and “PaperlessBilling”.

 
From the three independent variables we used in decision tree modeling, Contract comes out to be the most important variable to predict customer churn or not churn.
After analyzing the decision tree model, if any customer is in a one-year long contract and not using the PaperlessBilling, then this particular customer is unlikely to churn.
Apart from this, if any customer is in a month-to-month contract, and comes under the 0-12 month tenure, plus also using PaperlessBilling, then this customer is more likely to churn.
We are using the same decision tree model to create confusion matrix table and use it to make prediction. 
Now, we need to check the accuracy of our decision tree model, and which comes out be 0.7604.
The accuracy level is hardly improved. Let’s use Random Forest to check if we can do better or not.
Random Forest
“randomForest” function is used to create Random Forest Model.
The error rate is relatively low when predicting “No”, that means the prediction is pretty good when predicting “no”.
And the error rate is much higher when predicting “Yes”, that means the prediction is not good when predicting “Yes”.
Confusion Matrix requires the factors to have exactly the levels, for this we are converting “0” and “1” to “No” and “Yes” respectively.
Random Forest Error Rate
 
We plot our Random Forest Model, to determine the number of trees. The OOB error rate decreases as the number of trees increases. And then becomes almost constant. After 100 to 200 trees, we cannot able to decrease the OOB error rate.



Tune the Random Forest model to check the OOB error rate
 
We used “tuneRF” function to tune the Random Forest Model, after plotting the tuning Random Forest model, we can easily say that as the “mtry” increases, the OOB error rate decreases. 
Fitting the Random Forest Model again after tuning
The OOB Error Rate decreases to 19.7% from 20.11%.
Making predictions and confusion matrix again after tuning
The accuracy did not increase but the sensitivity improved, compared with the initial Random Forest Model before tuning.





Conclusion
The variables like the Tenure, Contract and PaperlessBilling are the important decision-making factor for the customer churn situation. Customers who are in a month-to-month contract, with PaperlessBilling and within 0-12 month tenure, are more likely to churn. Apart from this, customers who are in more than one-year contract and not using PaperlessBilling are unlikely to churn.
Fitting all the three models gives us the accuracy rate for each model. The accuracy rate of Logistic Regression Model is 79.98%, the accuracy rate of Random Forest before tuning comes out to be 79.36% and after we tune the Random Forest Model, the accuracy rate marginally increases to 79.74%. The accuracy rate for Decision Tree in all the three models comes out to be the lowest, i.e., 76.04%.
To conclude, we can say that the Logistic Regression and the Random Forest have provided better results and gives the better accuracy rate than the Decision Tree for the problem of predicting the Customer Churn in the future.






References:
[1.] Statistics Solutions, What is Logistic Regression? http://www.statisticssolutions.com/what-is-logistic-regression/
[2.] Medcalc, (2018), Logistic Regression https://www.medcalc.org/manual/logistic_regression.php
[3.] Stat 504, (2018), Lesson 6: Logistic Regression https://onlinecourses.science.psu.edu/stat504/node/149/
[4.] Prashant Gupta (17th May, 2017) Decision Trees in Machine Learning https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052
[5.] Niklas Denges (22nd Feb 2016) The Random Forest Algorithm https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd
